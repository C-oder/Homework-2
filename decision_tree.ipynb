{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-1",
   "metadata": {},
   "source": [
    "# Decision Tree for Banknote Authentication Dataset\n",
    "\n",
    "This notebook demonstrates the application of a decision tree classifier on the Banknote Authentication Dataset from the UCI Machine Learning Repository. Below are the detailed steps and explanations for Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Image Statistical Measures\n",
    "\n",
    "### Variance\n",
    "- Measures the spread of pixel intensity values around the mean.\n",
    "- High variance: wide spread; Low variance: values clustered near the mean.\n",
    "\n",
    "### Skewness\n",
    "- Measures the asymmetry of the intensity distribution.\n",
    "- Positive skew: longer tail on the right (more high-intensity values).\n",
    "- Negative skew: longer tail on the left (more low-intensity values).\n",
    "\n",
    "### Kurtosis\n",
    "- Measures the 'tailedness' or peakedness of the distribution.\n",
    "- High kurtosis: heavy tails (more outliers); Low kurtosis: light tails (flatter distribution).\n",
    "\n",
    "### Entropy\n",
    "- Measures the randomness or complexity of the image.\n",
    "- Higher entropy: more detail and variation; Lower entropy: simpler, more uniform image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore the Dataset\n",
    "\n",
    "The dataset contains 1372 instances, 4 numerical features, and a binary class label (0: fake, 1: authentic). In this section, data is loaded from the provided ZIP file, and a comment is provided at the end on whether the decision tree algorithm is suitable for these features.",
    "\n",
    "**Decision Tree Suitability:**\n",
    "- If the features are numerical and the target is binary, decision trees is a suitable choice.\n",
    "- Decision trees do not require feature scaling and provide interpretable decision rules.\n",
    "- However, overfitting may occur without proper tuning, so hyperparameter adjustments are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "\n",
    "# For model building and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the dataset from the zip file\n",
    "# The .zip file is named 'banknote+authentication.zip' and contains a file named 'data_banknote_authentication.txt'\n",
    "zip_file = 'banknote+authentication.zip'\n",
    "with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "    # List files to see the content\n",
    "    print(z.namelist())\n",
    "    # Adjust the filename below if needed\n",
    "    with z.open('data_banknote_authentication.txt') as f:\n",
    "        df = pd.read_csv(f, header=None, names=['variance', 'skewness', 'kurtosis', 'entropy', 'class'])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
{
   "cell_type": "markdown",
   "id": "comment",
   "metadata": {},
   "source": [
    "### Comment on Decision Tree Suitability\n",
    "\n",
    "The dataset has a relatively small number of features (4 numerical features) and a binary target. Decision trees are WELL SUITED for such datasets because:\n",
    "\n",
    "- They can handle numerical features easily.\n",
    "- They do not require feature scaling.\n",
    "- They provide interpretable decision rules that can be visualized.\n",
    "\n",
    "However, decision trees can be prone to OVERFITTING if not properly tuned. In that case, we should experiment with different hyperparameters (such as `max_depth` and `min_samples_split`) to balance model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Data Splitting, Model Training, and Evaluation\n",
    "\n",
    "We split the data into training (80%) and testing (20%) sets, then train a DecisionTreeClassifier using different hyperparameters. The model is evaluated using accuracy, precision, recall, and F1-score, and a confusion matrix is also plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Split into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Instantiate the DecisionTreeClassifier with initial hyperparameters\n",
    "clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, criterion='gini', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the Decision Tree and Discuss Interpretability\n",
    "\n",
    "We use `plot_tree()` to visualize the decision tree. The depth of the tree is crucial for interpretability:\n",
    "- **Shallow Tree:** Easier to interpret but might miss complex patterns.\n",
    "- **Deep Tree:** Can capture complex relationships but may be too complex to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_tree_cell",
   "metadata": {},
   "outputs": [],
   "source": [
 "# Visualize the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=['Fake', 'Authentic'], filled=True, rounded=True, fontsize=10)\n",
    "plt.title('Decision Tree Visualization')\n",
    "plt.show()\n",
    "\n",
    "# It's possible to experiment with different max_depth values and observe the trade-off between model complexity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## Step 5: Feature Importance\n",
    "\n",
    "We extract and plot the feature importances from the trained model. This shows which features contribute most to the classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feat_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = clf.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feat_importances = pd.DataFrame({'Feature': features, 'Importance':importances}).sort_values(by='Importance', ascending=False)\n",
    "print(feat_importances)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_importances, palette='viridis')\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## Step 6: Remarks\n",
    "\n",
    "After training and evaluating the decision tree, here are some reflections:\n",
    "\n",
    "- **Interpretability:** The decision tree provides clear decision rules, making the model transparent.\n",
    "- **Performance:** With proper hyperparameter tuning, the decision tree performs well on the dataset. However, its simplicity may cause overfitting if not controlled.\n",
    "- **Overall Suitability:** Given the dataset's small number of features and binary classification task, the decision tree is a good choice. For higher accuracy or more complex data patterns, ensemble methods like Random Forests or Gradient Boosting might be explored.\n",
    "\n",
    "**Answer:** I believe the decision tree is a good model for the Banknote Authentication dataset because it handles numerical features well, requires minimal data preprocessing, and offers an interpretable set of decision rules. However, its tendency to overfit makes it necessary to carefully tune hyperparameters. For further improvement, ensemble methods might be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has walked through the entire process of loading the Banknote Authentication dataset, training and evaluating a decision tree classifier, visualizing the model, and extracting feature importances. Additionally, it provides remarks, comments on the model's suitability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}